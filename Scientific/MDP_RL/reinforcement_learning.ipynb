{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<div align=center>\n",
    "\t\t\t<font face=\"XB Niloofar\" size=5>\n",
    "\t\t\t\t<p></p>\n",
    "\t\t\t\t<p></p>\n",
    "«به نام خدا»\n",
    "\t\t\t\t<p></p>\n",
    "\t\t\t</font>\n",
    "\t\t\t<p></p>\n",
    "\t\t\t<br />\n",
    "            <img src=\"images/logo.png\" style=\"border-radius: 25px\" width=\"400\">\n",
    "            <img src=\"images/banner.png\" width=\"250\">\n",
    "\t\t\t<br />\n",
    "زمستان ۱۳۹۷\n",
    "\t\t</div>\n",
    "\t\t<hr/>\n",
    "\t\t<font color=#029C8B size=6>\n",
    "\t\t\t<br />\n",
    "\t\t\t<div align=center>\n",
    "               محتوای علمی یازدهمین نبرد هوش مصنوعی شریف\n",
    "            </div>\n",
    "\t\t</font>\n",
    "        <font color=#E7234D size=6>\n",
    "\t\t\t<br />\n",
    "\t\t\t<div align=center>\n",
    "                 «بخش چهارم: یادگیری تقویتی»\n",
    "            </div>\n",
    "\t\t</font>\n",
    "\t\t<br />\n",
    "\t\t<hr />\n",
    "\t\t<style type=\"text/css\" scoped>\n",
    "        p{\n",
    "        border: 1px solid #a2a9b1;background-color: #f8f9fa;display: inline-block;\n",
    "        };\n",
    "        </style>\n",
    "\t\t<div>\n",
    "\t\t\t<h3>فهرست مطالب</h3>\n",
    "\t\t\t<ul style=\"margin-right: 0;\">\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#start\">\n",
    "                        شروعی مختصر و مفید\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#what\">\n",
    "                        یادگیری تقویتی چیست و چرا\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#qlearning\">\n",
    "                        ایده‌ی بدیهی اول، گشت تصادفی\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#try\">\n",
    "                        تلاشی برای دقیق‌تر کردن شرط‌های لازم برای این الگوریتم\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#simple\">\n",
    "                       یک الگوریتم ساده: گشت تصادفی به همراه بروزرسانی Q مقدارها\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#feature\">\n",
    "                    عه! پس کلا مساله حل شد و تمام؟\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "                <li>\n",
    "\t\t\t\t\t<a href=\"#packman\">\n",
    "                    جدی جدی وقت یادگیری یک بازیه!\n",
    "                    </a>\n",
    "\t\t\t\t</li>\n",
    "        </div>\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"start\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "شروعی مختصر و مفید\n",
    "        </font>\n",
    "        <p></p>\n",
    "        با سلام! وقت کم است و اعمال بسیار! بیایید هر چه سریعتر با اجرای دستور زیر وارد پوشه‌ی پروژه‌ی سوم درس هوش مصنوعی شوید تا آموزش یادگیری تقویتی را شروع کنیم.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Project3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"start\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "        <p></p>\n",
    "      با اجرای تکه‌ی کد زیر می‌توانید کد‌ها را در همین یادداشت تغییر دهید و نتیجه را در عمل ببینید.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"what\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "یادگیری تقویتی چیست؟\n",
    "        </font>\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "        در یادداشت قبلی، با\n",
    "         mdp\n",
    "        ها آشنا شدیم.\n",
    "        هدف ما در این یادداشت این است که اگر ما محیط اطراف را یک  mdp \n",
    "        در نظر بگیریم، چگونه می‌توانیم الگوریتمی بنویسیم که به صورت بهینه در این محیط حرکت کند؟\n",
    "        در ابتدا دقت کنید که اگر خصوصیات mdp\n",
    "        را به صورت کامل داشته‌باشیم،‌می‌توانیم مسیر بهینه را مشخص کنیم.\n",
    "        اما چطور باید خصوصیات mdp را آرام آرام\n",
    "        به دست بیاوریم و آن‌ها را محاسبه کنیم؟\n",
    "        الگوریتم ما باید با محیط تعامل کند  و کم کم خصوصیات محیط را یاد بگیرد\n",
    "        روش‌های زیادی برای یاد گرفتن mdp و به دست آوردن\n",
    "        استراتژی بهینه از روی داده‌های به دست آمده از این تعامل با محیط وجود دارند. \n",
    "        یکی از معروف‌ترین این الگوریتم‌ها به\n",
    "       Q-learning\n",
    "       مشهور است.\n",
    "       این الگوریتم به صورت مستقیم به محاسبه‌ی \n",
    "        Q-مقدار‌ها می‌پردازد.\n",
    "       در واقع این الگوریتم محیط\n",
    "        gridworld\n",
    "        تحویل می‌گیرد و سپس از روی آن \n",
    "        Q-مقدار‌ها \n",
    "        را یاد می‌گیرد.\n",
    "         اگر از یادداشت قبل به خاطر داشته‌باشید، می‌دانید که با داشتن\n",
    "        Q-مقدار‌ها\n",
    "        محاسبه‌ي بهترین کنش یا عمل در هر لحظه بسیار ساده است. در ادامه به الگوریتمی برای چگونه یاد گرفتن این مقادیر می‌پردازیم.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"what\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "بازی اندکی با محیط\n",
    "        </font>\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "     بهتر است قبل از هر چیز، اندکی با محیط gridworld آشنا شوید.\n",
    "        به این منظور بهترین راه این است که آن را اجرا کنیم و کمی هم در آن بازی کنید. دقت داشته باشید که پارامتر‌های ورودی را می‌توانید به دلخواه تغییر دهید.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py --noise 0.2 --livingReward 0 -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"qlearning\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "یاد گرفتن Q-مقدار‌ها \n",
    "        </font>\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "        بنابر آنچه تا اینجا گفتیم، اگر ما \n",
    "       Q\n",
    "        مقدار‌ها  را داشته باشیم، می‌توانیم به راحتی تصمیم بهینه را در یک وضعیت مشخص کنیم.\n",
    "        اما چطور می‌توانیم \n",
    "        Q\n",
    "        مقدار‌ها را حساب کنیم؟\n",
    "        <p></p>\n",
    "        یک الگوریتم بسیار ساده این است که از یکی از وضعیت‌ها شروع کنیم و سپس به صورت تصادفی یکی از اعمالی را که در اختیار ماست انتخاب کنیم و آن را انجام دهیم. \n",
    "        بگذارید در ابتدا یک اجرا از این الگوریتم را با همدیگر ببینیم:\n",
    "        در اجرایی که در زیر مشاهده می‌کنید، الگوریتم هوشمند ما به صورت رندوم ۲۰ بار در \n",
    "        gridworld\n",
    "        بازی می‌کند.\n",
    "        البته قابل توجه است که متاسفانه تصادفی بازی کردن، چیزی نیست که به آن هوشمندی بگو‌ییم. اما در ادامه خواهید دید که همین  بازی تصادفی هم در نهایت منجر به درک دقیقی از محیط می‌شود.\n",
    "         چگونه از این حرکات تصادفی که در زیر می‌بینید، عاملی هوشمند سر در خواهد آورد؟\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -k 20 -a random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "سوالی که در این‌جا مطرح است این است که ما از چنین بازی تصادفی‌ای چه چیزی را می‌توانیم یاد بگیریم؟\n",
    "بیایید کمی با هم دیگر فکر کنیم. ما چه چیزی را می‌خواهیم بدانیم؟ ما در هر وضعیت می‌خواهیم بدانیم که اگر یک \n",
    "کنش از بین کنش‌هایی که در اختیار داریم را انتخاب کنیم و پس از دریافت جایزه‌ی آن کنش دیگر بهینه عمل کنیم در انتها به چه میزان\n",
    "جایزه دست خواهیم یافت. البته دقت دارید که در اینجا، منظور از جایزه، امید ریاضی جایزه است.\n",
    "در واقع توصیف دقیق‌تر این است که می‌خواهیم بدانیم در صورتی که یک کنش را انجام دهیم و از آن پس بهینه عمل کنیم،\n",
    "امید ریاضی جایزه‌ای که به آن دست پیدا خواهیم کرد چه مقداری است.\n",
    "اما چه باید بکنیم؟\n",
    "بیایید با هم روشی را برای آشنایی با محیط بسازیم. در ابتدا از روش‌های بسیار ابتدایی شروع می‌کنیم تا روش‌های بهتر و بهتری را خلق کنیم.\n",
    "<p></p>\n",
    "\n",
    "<font color=#FF7500 size=6>\n",
    "ایده‌ی بدیهی اول: گشت تصادفی \n",
    " </font>\n",
    "ما در اینجا شوربختانه هیچ شهودی نسبت به احتمالات یا جایزه‌ها نداریم. بنابراین تنها راه چارهٔ ما گشت زدن در محیط است.\n",
    "یک راه بسیار ساده شاید این باشد که بارها و بارها یک مسیر را طی‌کنیم و مسیر‌های مختلف را امتحان کنیم تا آرام آرام شهودی نسبت به محیط دست پیدا کنیم.\n",
    "فرض کنید که اینگونه شروع کنیم. در یک وضعیت قرار بگیریم و به صورت تصادفی به سمت بالا برویم.\n",
    "و همینطور تصادفی حرکت کنیم  تا وقتی که \n",
    "بازی تمام شود. یا در واقع یک اپیزود از بازی تمام ‌شود. فرض کنید که در این اجرای از بازی ما به جایزه‌ی \n",
    "100\n",
    "رسیده باشیم.\n",
    "حالا شهود شما در مورد وضعیت اول چیست؟\n",
    "اولین چیزی که به ذهن میر‌سد این است که با خود بگوییم اگر به سمت بالا برویم، می‌توانیم 100\n",
    "امتیاز جمع کنیم. آیا این شهود درست است؟\n",
    "این شهود اشکال‌های زیادی دارد\n",
    ":\n",
    "\n",
    "<ul>\n",
    "<li>\n",
    "ما بهینه عمل نکرده‌ایم. ما صرفا در یک اجرا به سمت بالا، به امتیاز 100 دست یافته‌ایم. این به هیچ وجه دلیلی بر این نیست که در \n",
    "حرکت بهینه به سمت بالا ما نتوانیم بیشتر از صد امتیاز بگیریم.\n",
    "در واقع ما پس از حرکت به سمت بالا تصادفی عمل کرده‌ایم و برای همین می‌توان گفت که شاید حرکت هوشمندانه‌تری موجود باشد که ما رابه \n",
    "جایزه‌ای بیش از 100 برساند\n",
    "(دقت داشته باشید که منظور امید ریاضی جایزه است).\n",
    "</li>\n",
    "<li>\n",
    "شاید اتفاقا در همین حرکت تصادفی هم بسیار خوش‌شانس بوده‌ایم که امتیاز 100\n",
    "را بدست آورده‌ایم.\n",
    "شاید در عمل در این محیط اگر در اول به سمت بالا حرکت کنیم و سپس بهینه حرکت کنیم،\n",
    "نهایتا به امیدریاضی \n",
    "80 بتوانیم برسیم.\n",
    "چگونه می‌خواهیم مطمئن شویم که چنین اتفاقی رخ نداده است؟\n",
    "</li>\n",
    "</ul>\n",
    "خبر خوب این است که الگوریتمی وجود دارد که به ما این اطمینان را می‌دهد که تمام\n",
    "Q\n",
    "مقدار‌ها را به طور دقیق پیدا کند.\n",
    "البته منظور من از دقیق این است که این الگوریتم ما به ما اطمینان می‌دهد که در یک محیط مارکوف،\n",
    "در صورتی که تا زمان بی‌نهایت اجرا شود مقادیری را به عنوان \n",
    "Q\n",
    "مقدار‌ها به ما گزارش می‌دهد که به\n",
    "Q\n",
    "مقدارهای دقیق همگرا هستند.\n",
    "به نظر شما چنین الگوریتمی، چگونه عمل می‌کند؟\n",
    "بیایید کمی فکر کنیم بالاخره شاید ما هم به عنوان  مهندسان کامپیوتر و کسانی که اندکی از زیبایی‌ها ریاضی چشیده‌اند بتوانیم خودمان فکر کنیم و لذت خلق یک راه‌حل را بچشیم. من هم به عنوان کسی که این الگوریتم را دیده‌ام به شما اطمینان می‌دهم که این الگوریتم از مفاهیم بسیار پیچیده و خاصی استفاده نمی‌کند که شما با آن آشنا نباشید.\n",
    "در واقع اگر کمی دقت کنید تمام دانش لازم برای خلق آن تا به اینجا به شما یاد داده شده است و تنها کافی است که بخواهید از آن استفاده کنید تا بتوانید این الگوریتم را خودتان برای خودتان ابداع کنید.\n",
    "\n",
    "<p></p>\n",
    "یکی از کار‌های خوب و مناسب برای کشف راه‌حل‌های یک مساله، این است که در ابتدا توجه کنیم  که آن راه‌حل باید چه خصوصیاتی داشته باشد. منظور این است که خیلی از مواقع ما دقیقا نمی‌توانیم راه‌حل را مشخص کنیم اما می‌توانیم شهودی داشته باشیم که آن راه‌حل باید چه خصوصیاتی داشته باشد؟\n",
    "\n",
    "<p></p>\n",
    "خواهشا در اینجا کمی توقف کنید و کمی فکر کنید. به نظر شما این چنین الگوریتمی چه خصوصیاتی باید داشته باشد؟\n",
    "چه شرایطی را اگر رعایت نکند به این معناست که قطعا نمی‌تواند ادعا کند که در صورت اجرا شدن تا بی نهایت در یک محیط مارکوف مقادیری که گزارش می‌کند به \n",
    "Q\n",
    "مقدار‌های صحیح و درست همگرا شوند؟\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"try\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "تلاشی برای دقیق‌تر کردن شرط‌های لازم برای این الگوریتم\n",
    " </font>\n",
    "شاید اولین ایده‌ای که به ذهن یک نفر برسد این است که ما باید در هر وضعیت، حتما تمام کنش‌ها را حداقل یکبار امتحان کنیم زیرا ممکن است آن یک کنش جایزه‌ای بسیار بزرگ داشته‌باشد و در واقع با اختلاف بسیار زیاد با بقیه‌ی جوایز موجود در \n",
    "    آن محیط مارکوف از آن‌ها جلو باشد. تا وقتی آن را امتحان نکرده‌باشیم نمی‌توانیم نظر دقیقی در مورد آن محیط بدهیم.\n",
    "    بیاید فرض کنیم که یک بازی ویدیوی در جلوی ماست:\n",
    "    <img src = \"images/chicken_invaders.jpg\">\n",
    "    نمی‌دانم که با این بازی آشنایی دارید یا نه! اما در این بازی شما در هر وضعیت می‌توانید یکی از دکمه‌های کیبرد را فشار دهید و یا موس را کلیک راست یا چپ کنید. \n",
    "    هدف شما هم این است که مرغ‌هایی را که در فضا می‌خواهند حکومت مرغی تشکیل دهند از\n",
    "    بین ببرید.\n",
    "   اکثر کسانی که این بازی را انجام می‌دهند تنها با کلید‌های \n",
    "    راست و چپ و بالا و پایین اتکا می‌کنند و چند کلیک موس.\n",
    "    از کجا نمی‌دانیم که مثلا با فشردن دکمه‌ی t\n",
    "    که به هر حال یکی از کنش ‌های ممکن ما با یک برنامه‌ی کامپیوتری است، تمام مرغ‌ها در آن کشته نمی‌شوند؟\n",
    "    این چیزی نیست که تا آن را امتحان نکنیم بدانیم.\n",
    "    این موضوعی خیلی مهم است.\n",
    "    پس ما به یک شرط بسیار مهم رسیدیم.\n",
    "    <b>\n",
    "        ما در هر وضعیت باید تمام کنش‌ها را حداقل یکبار بررسی کرده‌ باشیم.\n",
    "    </b>\n",
    "    <p></p>\n",
    "    آیا شرط دیگری هست که به نظر شما حتما لازم باشد که چنین برنامه‌ای داشته باشد؟\n",
    "    برای مثال من خودم وقتی به این فکر می‌کنم که آیا می‌شود یک کنش را تعداد متناهی بار آزمایش کرد؟\n",
    "    برای مثال فرض کنید که ما دو سکه‌ی شانس داریم و هر بار می‌توانیم یکی از آن‌ها را بیندازیم و اگر سمت شکل دار سکه بیاید، امتیازی را دریافت کنیم و بعد از ده بار بازی تمام شده است.\n",
    " چیزی که دراینجا مهم است این است که احتمال آمدن هیچ کدام از سمت‌ها را نمی‌دانیم.\n",
    "    در واقع این سکه‌ها سکه‌های عادلانه نیستند که هر سمت آن‌ها تنها به احتمال نیم بیاید. بلکه مثلا ممکن است سکهٔ سمت راست به احتمال\n",
    "    هشت دهم بیاید یا به احتمال سه دهم بیاید.\n",
    "    <div class=\"row\">\n",
    "     <div class=\"column\">\n",
    "        <img src=\"images/fishcoin.png\" alt=\"fishcoin\" style=\"width:20%;float:left;\">\n",
    "     </div>\n",
    "     <div class=\"column\">\n",
    "        <img src=\"images/leafcoin.jpeg\" alt=\"leafcoin\" style=\"width:20%;float:right;\">\n",
    "     </div>\n",
    "   </div> \n",
    "        حال فرض کنید که جایزهٔ هر بار انداختن هر سکه هم مشخص است و اگر سمت شکل دار سکه بیاید ما یک سکهٔ طلا دریافت می‌کنیم.\n",
    "    به نظر شما اگر تنها به تعداد متناهی بار یکی از سکه‌ها را بیاندازیم می‌توانیم مطمئن باشیم که \n",
    "    احتمال دقیق را محاسبه کرده‌ایم؟\n",
    "    جواب این سوال کمی به دانش ریاضی نیاز دارد اما به هر حال باید بدانیم که اگر بخواهیم احتمال شکل آمدن مثلا سکهٔ سمت راست را بدانیم باید  آن را نامتناهی بار بیاندازیم تا بتوانیم با تقسیم تعداد بار‌هایی که شکل آمده است بر تعداد بارهایی که عدد آمده است احتمال شکل آمدن را حساب کنیم.\n",
    "    در واقع\n",
    "    $$\\frac{num(picturs)}{num(tries)}$$\n",
    "    وقتی به احتمال واقعی همگرا می‌شود که تعداد بار‌های سکه انداختن به بی‌نهایت برسد.\n",
    "    پس در واقع می‌توان گفت\n",
    "    <b>\n",
    "    الگوریتم ذکر شده هر چه باشد باید نامتناهی بار هر کنش را ارزیابی ‌کند.\n",
    "    </b>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"simple\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "یک الگوریتم ساده: گشت تصادفی به همراه بروزرسانی Q مقدار‌ها\n",
    " </font>\n",
    "فرض کنید که ما در یک وضعیت قرار می‌گیریم و سپس در هر مرحله به صورت تصادفی یک کنش را در محیط انجام می‌دهیم تا بازی تمام شود. و این  عمل را نامتناهی بار انجام می‌دهیم.\n",
    "    دقت کنید که این الگوریتم هر دو شرط ذکر شده در بالا را دارد. اما سوال اصلی این است: چگونه \n",
    "    Q \n",
    "    مقدار‌ها را بروزرسانی‌کنیم؟\n",
    "    در واقع چگونه با هر بار انتخاب یک کنش و مشاهدهٔ امتیاز‌ها\n",
    "    حدس خودمان از \n",
    "    Q \n",
    "    مقدار ها را بروزرسانی کنیم؟\n",
    "    یک ایدهٔ ابتدایی این است:\n",
    "   فرض کنید در وضعیت \n",
    "    $S$\n",
    "    هستیم وکنش\n",
    "    $a$\n",
    "    را از بین کنش‌های موجود انتخاب می‌کنیم.\n",
    "    سپس\n",
    "    $R(s,a,s')$\n",
    "    را دریافت می‌کنیم \n",
    "   . \n",
    "   به نظر شما چگونه می‌توانیم مقدار\n",
    "   $Q(s,a)$\n",
    "   را بروزرسانی کنیم تا شهود دقیق‌تری از مقدار آن داشته باشیم؟\n",
    "   <p></p>\n",
    "   یک ایدهٔ ساده این است که:\n",
    "   $$Q_{new}(s,a) = R(s,a,s') + \\gamma \\times Val(s')$$\n",
    "   یا به عبارت دیگر\n",
    "   $$Q_{new}(s,a) = R(s,a,s') + \\gamma \\times max_{a'\\in A(s)} Q(s', a')$$\n",
    "   نه نه! این اصلا چیز ترسناکی نیست و در واقع بسیار هم ساده است. کل حرفی که این عبارت ریاضی می‌زند این است که حالا که این مقدار از جایزه را با انتخاب این کنش دریافت کردم، اگر در \n",
    "   وضعیت \n",
    "   $s'$\n",
    "   هم بهترین انتخاب‌ها را داشته باشم\n",
    "   (\n",
    "   که در واقع همان قسمت عجیب‌غریب‌نمای\n",
    "   عبارت ریاضی بالاست که در واقع \n",
    "   بیان می‌کند که کدام کنش در وضعیت \n",
    "   $s'$\n",
    "   بهترین امیدریاضی را با توجه به حدس‌های ما تا الآن\n",
    "   ایفا می‌کند و سپس \n",
    "   مقدار آن امید ریاضی را برمی‌گرداند\n",
    "  )\n",
    "  چه امیدی می‌توانم با حدس‌های الآنم داشته باشم.\n",
    "  <p></p>\n",
    "  این عبارت، عبارت بسیار خوبی برای بروزرسانی است اما یک جای کار این عبارت می‌لنگد.\n",
    "  به نظر شما این نحو از بروزرسانی چه مشکلی می‌تواند داشته باشد؟\n",
    "  بیایید با همدیگر ببینیم:\n",
    "   \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    estimatedQ = reward + self.discount * self.computeValueFromQValues(nextState)\n",
    "    self.qValues[(state,action)] = estimatedQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"random_learb\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "دقت کنید که پارامتر ورودی\n",
    "discount\n",
    "    پاس داده شده در واقع همان \n",
    "    gamma($\\gamma$)\n",
    "   است که ضریب کاهش پاداش است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 30 -m --discount 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "مشکلات این نحوه از بروزرسانی شاید در ابتدا به نظر نرسند اما با کمی بررسی متوجه آن‌ها خواهید شد. خواهش ما این است که ابتدا سعی کنید خودتان مشکلات آن را بیابید و سپس به موارد زیر که به عقل ما رسیده است توجه کنید.\n",
    "<p></p>\n",
    "    از نظر ما بزرگترین مشکل این نحوه از بروزرسانی توجه به آخرین اتفاق در محیطی است که اتفاقات تصادفی هستند. بگذارید با استفاده از یک مثال ساده موضوع را شرح دهیم. همانطور که قبلا بررسی شده است در بازی\n",
    "   gridWorld\n",
    "   خانهٔ سمت راست بالا امتیاز یک دارد. خانهٔ سمت راست آن را در نظر بگیرید. حرکت به سمت راست در این خانه باید امیدریاضی بالایی داشته باشد.\n",
    "    چرا؟\n",
    "    چونکه در نزدیکی یک خانهٔ یک امتیازی است و اگر حرکت به سمت راست موفقیت آمیز باشد ما \n",
    "    در نزدیکی کسب یک امتیاز هستیم. اما حالا فرض کنید که موفقیت آمیز نباشد. بدین معنا که همانطور که می‌دانیم هر حرکت در\n",
    "    gridworld\n",
    "    با یک خطای اندک انجام می‌شود. ما به خانهٔ پایین می‌رویم و باز هم شانس بازگشت به خانهٔ بالا را داریم.\n",
    "    اما نمونهٔ زیر را نگاه کنید که در هنگام یادگیری اتفاق افتاده است.\n",
    "    در ابتدا وضعیت به صورت زیر بوده است:\n",
    "    <img src = \"images/direct_mistake.png\">\n",
    "    حال ربات ما در وضعیت بالا تصمیم می‌گیرد به سمت راست برود اما دراینجا به سمت پایین می‌لغزد. همانطور که می‌بینید ناگهان ارزش حرکت به سمت راست در آن خانه برای او به شدت پایین می‌آید.\n",
    "    <img src = \"images/direct_mistake_2.png\">\n",
    " اما چنین اتفاقی منطقی نیست زیرا قبل از این اتفاق بار‌ها همین ربات\n",
    "از طریق همین حرکت در همین خانه به خانهٔ یک رفته بود. این کاهش شدید در ارزش منطقی نیست.\n",
    "    در واقع به صورت خلاصه مشکل این روش یادگیری\n",
    "    <b>\n",
    "        تصمیم گرفتن بر اساس آخرین اطلاع و فراموش کردن گذشته در محیطی تصادفی است که اتفاقات تصادفی بسیار می‌افتند و آخرین تجربه می‌تواند نمایانگر بخش کوچکی از احتمالات در آن محیط باشد.      \n",
    "    </b>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "از مشکلات روش بالا گفتیم. اما چه کنیم تا تابعی منطقی‌تر برای بروزرسانی داشته باشیم؟\n",
    "دوست عزیز ایده‌ای بزن که به این ره گرفتار گشته‌ایم.\n",
    "البته ایدهٔ حل این مساله سخت یا پیچیده نیست. بلکه ایده‌ای بسیار ساده است.\n",
    "کافی است خاطره‌ای را که از قبل در ذهنمان نسبت به \n",
    "$Q(s,a)$\n",
    "داشتیم به صورت کامل فراموش نکنیم و بلکه آن را در خاطر خودمان داشته باشیم.\n",
    "به عبارت جدید بروزرسانی دقت کنید.\n",
    "$$Q_{new}(s,a) = \\alpha \\times [ R(s,a,s') + \\gamma \\times Val(s') ] + (1-\\alpha) \\times Q_{old}(s,a) $$\n",
    "یا به عبارت دیگر\n",
    "$$Q_{new}(s,a) = \\alpha \\times [ R(s,a,s') + \\gamma \\times max_{a'\\in A(s)} Q(s', a')] + (1-\\alpha) \\times Q_{old}(s,a)$$\n",
    "همانطور که می‌بینید در این جا ما \n",
    "    با یک ضریب \n",
    "    $\\alpha$\n",
    "    خاطرات جدیدمان را تاثیر می‌دهیم و با ضریب\n",
    "    $1- \\alpha$\n",
    "    خاطرات گذشته را به یاد می‌آوریم.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    estimatedQ = self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState)) + (1-self.alpha)*self.qValues[(state,action)]\n",
    "    self.qValues[(state,action)] = estimatedQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"random_learb\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "دقت کنید که پارامتر ورودی\n",
    "learningRate\n",
    "    پاس داده شده در واقع همان \n",
    "    alpha($\\alpha$)\n",
    "    ی بحث شده در بالاست.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 30 -m --discount 0.9 --learningRate 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"random_learb\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "یادگیری تصادفی معهود:\n",
    " </font>\n",
    "همانطور که در زیر می‌بینیم، توانستیم با کمک همدیگر، الگوریتمی برای یادگیری\n",
    "Q-مقدار‌ها\n",
    "    به صورت کاملا اتوماتیک است. البته این الگوریتم، مشکلاتی دارد که راه‌حل‌هایی برای حل این مشکل‌ها ارائه می‌دهیم.\n",
    "    \n",
    "   \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 100 --epsilon 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "محاسبه‌ی حرکات بهینه با توجه به Q-مقدار‌ها\n",
    ":\n",
    "</font>\n",
    "حالا که Q-مقدار‌ها\n",
    "    را داریم چگونه می‌توانیم با استفاده از آن‌ها، عمل بهینه را محاسبه کنیم و آن را انجام بدهیم؟ \n",
    "    به عبارتی این همه \n",
    "    Q-مقدار یاد\n",
    "    گرفتیم که چه بشود؟\n",
    "پاسخ به این سوال بسیار ساده است. اگر متوجه دلیل پشت این پاسخ نمی‌شوید،‌یکبار دیگر تعاریف مربوط به \n",
    "    Q-مقدار‌ها\n",
    "    را مرور کنید. \n",
    "    \n",
    "  <div style=\"color:red\">\n",
    "   پاسخ: \n",
    "      فرض کنید که در وضعیت \n",
    "      $s$\n",
    "      هستیم،\n",
    "      حالا باید بین اعمال مختلف و ممکن انتخاب کنیم.\n",
    "       مجموعه اعمال ممکن را \n",
    "      $A$\n",
    "      بنامید،\n",
    "      حال کافی است عمل\n",
    "      $a$\n",
    "      را انتخاب می‌کنیم که\n",
    "      $Q(s, a)$\n",
    "      برای آن بزرگتر مساوی\n",
    "      $Q(s, a')$\n",
    "      به ازای هر\n",
    "      $a' \\in A$\n",
    "      باشد. توضیحات ریاضی پیچیده شد! خلاصه‌اش این است که وقتی به ازای هر عمل، ما فهمیده‌ایم که امید ریاضی پاداش چقدر است، می‌توانیم عمل با بالاترین امیدریاضی را انتخاب می‌کنیم.\n",
    "      \n",
    "      \n",
    "      \n",
    "  </div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"feature\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "عه! پس کلا مساله حل شد و تمام؟\n",
    " </font>\n",
    " همانطور که در بخش قبل مشاهده کردید، ما الگوریتمی را پیدا کردیم که می‌تواند تمامی \n",
    " Q مقدار‌ها را به درستی یاد بگیرد.\n",
    " در ابتدا،‌شاید فکر کنید که این همان نهایت آرزوهای ما در  یادگیری تقویتی است. پرونده‌ی یادگیری تقویتی و در واقع \n",
    " «چکش طلایی»\n",
    " تمام بازی‌های دنیا را کشف کرده‌ایم.\n",
    " \n",
    " اما متاسفانه خبر بدی برای شما دارم و آن هم این است که اینطور نیست. \n",
    " از شما می‌خواهم که کمی فکر کنید که چرا اینطور نیست؟\n",
    " واقعا مگر چیزی جز این لازم داریم  که تمام\n",
    " Q\n",
    " مقدار‌ها را به درستی یاد بگیریم؟ \n",
    " \n",
    " </br>\n",
    "در واقع با یاد گرفتن تمام\n",
    "Q\n",
    "مقدار‌ها، ما می‌توانیم در هر مرحله به صورت کاملا دقیق مشخص کنیم که کدامین عمل در این وضعیت، بهترین انتخاب است یا در واقع بیش‌ترین امیدریاضی سود را به ما می‌رساند.\n",
    " پس مشکل چیست؟\n",
    " مشکل در واقع در سرعت این الگوریتم یادگیری است. چیزی که به طور مشخص وجود دارد این است که در بسیاری از مسایل دنیای واقعی، یا حتی در بسیاری از مسایل دنیای غیرواقعی، مانند همین بازی‌ای که در مسابقه‌ي چالش هوش مصنوعی به شما ارایه شده است،\n",
    " فضای حالات به قدری بزرگ است که نمی‌توان برای تمامی حالات آن یک وضعیت در نظر گرفت و سپس در هر وضعیت آن اعمال مختلف را با یک الگوریتم تصادفی پیمایش کرد تا در عمل ببینیم که کدام حالت و کدام وضعیت می‌تواند بهتر باشد. به بازی بسیار ساده‌ي زیر نگاه کنید:\n",
    " <img src = \"images/mario-danger.jpg\">\n",
    " این بازی همان بازی سوپر ماریوی معروف است که احتمالا شما که نه اما کمی اگر به عقب برگردیم بسیاری از ما با آن خاطرات بسیار زیادی داریم. اما به هر حال چیزی که یک برنامه‌ی یادگیری تقویتی خوب باید از این صحنه یاد بگیرد احتمالا این است که وجود این وروجک‌های قهوه‌ای در سمت راست ماریو این را ایجاب می‌کنید که ماریو به سمت چپ حرکت کند. در واقع اگر بخواهیم با برنامه‌ي بالا آن را یاد بگیریم اگر در بازی به این مرحله برسیم، ممکن است برنامه‌ي  ما یکبار به سمت راست برود و بازی را ببازد تا بالاخره یاد بگیرد که با رفتن به سمت چپ می‌تواند بازی را ببرد. \n",
    " این که بسیار خوب و عالی است. پس چه اشکالی دارد؟\n",
    "  اشکال آن را با نگاه به شکل زیر متوجه خواهید شد:\n",
    " <img src = \"images/mario-danger-2.jpg\">\n",
    "در واقع باید یک موضوع ناراحت‌کننده را با شما مطرح کنم و آن هم این است که ما از دیدن وضعیت اول و یاد گرفتن اینکه خوب است در آن صحنه به سمت چپ برویم،‌هیچ چیز در مورد صحنه‌ي دوم و ایتن که خوب است در این صحنه هم به سمت چپ برویم یاد نگرفته‌ایم. \n",
    "چرا؟\n",
    "علت واضح است، زیرا این دو وضعیت کاملا متفاوت از یکدیگر هستند و در واقع برنامه‌ي ما هیچ شهودی ندارد که این دو برنامه چقدر شبیه به یکدیگر هستند. برای مثال وضعیت اول را وضعیت \n",
    "123\n",
    "شماره گذاری کرده  و یاد گرفته است در وضعیت 123 کدام حرکت بهتر است\n",
    "    اما وضعیت دوم را وضعیت 124 شماره گذاری کرده و حالا باید در آن هم سعی و خطا کند تا بفهمد که کدام برایش بهتر است.\n",
    "    نکته! این دو وضعیت متفاوت هستند،‌چون! اگر گفتید؟\n",
    "    چون که رنگ گل زرد وضعیت اول در دومی سبز شده است!\n",
    "    این برنامه‌ی یادگیری ما از عدم بهینگی سرشار است و حالا باید برای حل آن چه کاری انجام دهیم؟\n",
    "    چیزی که در پاسخ باید گفت این است که راه‌حل‌های بسیاری برای حل این مشکل ارایه شده‌اند که نمی‌توان یک راه را به عنوان \n",
    "    «چکش طلایی»\n",
    "    و همه‌کاره معرفی کرد. \n",
    "    اما به هر حال یکی از بهترین ایده‌های این کار این است که فضای مساله را  در فضای دیگری نمایش دهیم.\n",
    "    در واقع فرض کنید که با دیدن تصویر بازی سوپر ماریو در وضعیت بالا تعدادی خصیصه را از آن استخراج کنیم و سپس برای این فضای محدود شده، یادگیری تقویتی را اجرا کنیم.\n",
    "    شاید این توضیح برای شما در ابتدا کمی گنگ باشد اما مثال زیر همه چیز را بسیار روشن‌تر می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "در فضایی دیگر!\n",
    " </font>\n",
    "فرض کنید می‌خواهیم یک الگوریتم یادگیری تقویتی برای بازی زیر بنویسیم. این بازی البته بازی \n",
    "plants vs zombies\n",
    "است که بازی بسیار معروفی است. اما به عنوان یک توضیح کوتاه در این بازی ما در هر گام می‌توانیم یکی از گیاهانی را که  به تعداد کافی برای داشتن آن امتیاز داریم،‌از منوی بالا انتخاب کنیم و در یکی از خانه‌های محوطه‌ی چمنی آن را بکاریم. .\n",
    "    هدف این گیاه‌ها این است که جلوی ورود زامبی‌ها را به خانه‌ی ما بگیرند.\n",
    "<img src = 'images/pvz.jpg'>\n",
    "یک مدل‌سازی بسیار ساده و البته غیر کارا برای این بازی این است که هر وضعیت بازی را به عنوان مقادیر قرمز و سبز و آبی برای تمام پیکسل‌های بازی بدانیم. قطعا این نحوه از نگاه به بازی تمام اطلاعاتی را که در اختیار هست، برای ما نگاه می‌دارد. اما تعداد حالات مختلف بازی انقدر زیاد می‌شوند که در عمل برای یادگیری \n",
    "    Q\n",
    "    مقدار‌ها\n",
    "    به زمانی بسیار بسیار بسیار زیاد نیاز داریم.\n",
    " \n",
    " ما می‌توانیم یک مرحله‌ انتزاع ورودی را زیاد‌تر کنیم و در واقع ورودی‌ای خلاصه‌تر اما با حفظ جزییات داشته‌باشیم.\n",
    " در واقع در این مرحله‌ی خلاصه‌سازی از روش‌های بسیار زیادی می‌شود استفاده کرد.\n",
    " اما به عنوان یک خلاصه‌سازی بسیار ساده و دم‌دستی می‌توان تنها مکان گل‌ها در مختصاتی که کاشته شده‌اند، نوع آن گل و مکان زامبی‌ها را به عنوان ورودی در نظر گرفت.\n",
    "می‌توانیم یک مرحله انتزاع را به پیش‌ ببریم و برای مثال تنها برای هر پنج خط ورود زامبی‌ها، فاصله‌ی آن‌ها را بر حسب سه مقدار دور و متوسط و نزدیک دسته‌بندی کنیم.\n",
    "\n",
    "اینکه چه سطحی از انتزاع را انتخاب می‌کنیم کاملا در اختیار ماست. البته کار دیگری که می‌توانیم بکنیم این است که از یک شبکه‌ی عصبی یا روش‌های دیگر یادگیری بخواهیم که این مرحله از انتزاع را برای ما ایجاد کنند که البته در اینجا وارد آن‌ها نمی‌شویم اما در هر صورت یکی از کاربرد‌های شبکه‌های عصبی به دست آوردن انتزاع‌های پیچیده بدون کمک چندان انسانی است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "جست و جوی بیش‌تر یا استفاده از آنچه داریم؟\n",
    "</font>\n",
    "همانطور که در بخش الگوریتم‌های تصادفی مشاهده کردید، اگرچه ما الگوریتمی تصادفی داریم که \n",
    "    Qمقدار‌ها\n",
    "    را یاد می‌گیرد اما حدس بزنید که الگوریتم ما چه اشکالی دارد؟\n",
    "    اشکال الگوریتم ما در واقع این است که اگر آن را در دنیای واقع اجرا کنیم،\n",
    "    خواهیم دید که در واقع برنامه‌ی ما به صورت تصادفی عمل می‌کند. البته علت آن هم واضح است و آن این است که در حال عمل تصادفی است تا بتواند \n",
    "    Q-مقدار‌ها\n",
    "    را یاد بگیرد اما هیچ کاری با این یادگیری انجام شده انجام نمی‌دهد.\n",
    "   بنابراین همانطور که احتمالا خودتان هم فکر می‌کنید باید پس از یک سری حرکات تصادفی آرام آرام حرکات تصادفی را کم و کنیم  و به آنچه آموخته‌ایم تکیه کنیم.\n",
    "در اینجا دو روش از راه‌های ادغام این روش به صورتی ریاضی با الگوریتم‌های یادگیری ما است.\n",
    "روش ما بسیار ساده است. ما در ابتدا یک \n",
    "$\\epsilon$\n",
    "را که عددی به عنوان صفر و یک است انتخاب می‌کنیم.\n",
    "    برای مثال می‌توانیم فرض کنیم\n",
    "$\\epsilon = 0.1$.\n",
    "حال، در هر مرحله‌ی تصمیم‌گیری به احتمال\n",
    "$\\epsilon$\n",
    "    حرکتی کاملا تصادفی انجام می‌دهیم و به احتمال\n",
    "$1-\\epsilon$\n",
    "حرکتی را که با توجه به آنچه تا اینجا آموخته‌ایم، بهینه است، انجام می‌دهیم.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 100 --epsilon 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "همانطور که در مثال بالا مشاهده می‌کنید پس از مدتی، راه برای کسب امتیاز بهینه مشخص می‌شود و بنابراین دیگر اپسیلون با آن مقدار بالا چندان منطقی به نظر نمی‌رسد. در این جا است که از  روشی استفاده می‌کنند که آرام آرام مقدار اپسیلون را کاهش می‌دهد تا پس از مدتی کم کم جست و جو متوقف و راه بهینه را دنبال کنیم. اما دقت کنید که چنین کاری باعث میشود تا  دیگر از اینکه \n",
    "    Q\n",
    "    مقدار‌های صحیح را به دست می آوریم مطمین نباشیم.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    self.alpha = 0.5\n",
    "    estimatedQ = self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState)) + (1-self.alpha)*self.qValues[(state,action)]\n",
    "    self.qValues[(state,action)] = estimatedQ\n",
    "    self.epsilon = (self.epsilon)*0.999\n",
    "    print('epsilon is', self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 100 --epsilon 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    حتی می‌توانیم آرام آرام اعتمادمان نسبت به آنچه یاد گرفته‌ایم را بیش‌تر هم بکنیم  و بیشتر در یادگیری به آن اعتماد کنیم.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    estimatedQ = self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState)) + (1-self.alpha)*self.qValues[(state,action)]\n",
    "    self.qValues[(state,action)] = estimatedQ\n",
    "    self.epsilon = (self.epsilon)*0.999\n",
    "    self.alpha = (self.alpha*999+1)/1000\n",
    "    print(self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 100 --epsilon 0.99 --learningRate 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "می‌توانیم آنچه نوشته‌ایم را بر روی محیط بزرگتری امتحان کنیم:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run gridworld.py -a q -k 50 -n 0 -g BridgeGrid --epsilon 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "در نهایت، تابع را به آنچه در ابتدا بود برمی‌گردانیم! می‌توانید هرچقدر دلتان می‌خواهد آن را تغییر دهید:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    estimatedQ = self.alpha * (reward + self.discount * self.computeValueFromQValues(nextState)) + (1-self.alpha)*self.qValues[(state,action)]\n",
    "    self.qValues[(state,action)] = estimatedQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "جدی جدی وقت یادگیری یک بازیه!\n",
    "</font>\n",
    "در این قسمت می‌خواهیم با استفاده از الگوریتم‌هایی که یاد گرفتیم برای بازی\n",
    "pacman\n",
    "یک الگوریتم بنویسیم که بازی بهینه را یاد بگیرد. در ابتدا یک بار بازی پک من را اجرا کنیم.\n",
    "    دستور زیر در ابتدا دو هزار بار بازی را برای یادگیری پک‌من اجرا می‌کند و سپس  اپسیلون و آلفا را صفر کرده و ده بار از بازی را برای ما نمایش می‌دهد. ببینیم:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p PacmanQAgent -x 2000 -n 2010 -l smallGrid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "الگوریتم بالا بر روی یک نقشه‌ی کمی بزرگتر\n",
    "</font>\n",
    "بیایید همین بازی را بر روی نقشه‌ای کمی بزرگتر انجام دهیم:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p PacmanQAgent -x 2000 -n 2010 -l mediumGrid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "اشکال کجاست؟\n",
    "</font>\n",
    "فکر می‌کنید اشکال کار کجاست و چرا الگوریتم ما اینگونه ضعیف عمل می‌کند؟ در واقع  علت بزرگی استیت‌ها و تعداد حالات بسیار زیادی بازی پک‌من است. یکی از راه‌حل‌های این موضوع همانطور که در بخش‌های قبلی همین یادداشت گفته شد، کاهش تعداد وضعیت‌هاست. اما این‌جا به بررسی یک راه‌حل بسیار جالب برای حل مساله‌ي پک‌من می‌پردازیم. ما می‌خواهیم\n",
    "    $Q(s, a)$\n",
    "    را محاسبه کنیم و در واقع تمام این سختی‌هایی که متحمل می‌شویم برای محاسبه‌ی همین امر است. حال فرض کنید که \n",
    "    $$Q(s, a) = \\sum_{i}^{n} fـ{i}(s, a)\\times w_{i}$$\n",
    "    که در این فرمول \n",
    "    $f_{i}(s, a)$\n",
    "    تابعی است که وضعیت ورودی و عمل را می‌گیرد و یک عدد را بر می‌گرداند. در واقع این عدد یک ویژگی از بازی است.\n",
    "    حال پس از هر مرحله تفاوت بین آنچه رخ می‌دهد و آنچه انتظار داریم رخ بدهد را مشاهده می‌کنیم و با توجه به آن ضرایب\n",
    "    ، یعنی \n",
    "    $w_{i}$\n",
    "    ها را به گونه‌ای تغییر می‌دهیم که تخمین بهتری از\n",
    "    $Q(s, a)$\n",
    "    ها بزنند.\n",
    "    در واقع داریم که:\n",
    "    $$difference = (reward+\\gamma \\times \\underset{a'}{max} Q(s', a')) - Q(s, a)$$\n",
    "    $$w_{i} \\leftarrow w_{i}+\\alpha \\times difference \\times f_{i}(s, a)$$\n",
    "    احتمالا مساله‌ی اصلی در چنین راه‌حلی این است که چه ویژگی‌هایی را استخراج کنیم.\n",
    "    برای یک پیاده‌سازی ساده می‌توانید فایل\n",
    "    featureExtracotrs.py\n",
    "    را مشاهده کنید:\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile featureExtractors.py\n",
    "\"Feature extractors for Pacman game states\"\n",
    "\n",
    "from game import Directions, Actions\n",
    "import util\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def getFeatures(self, state, action):\n",
    "        \"\"\"\n",
    "          Returns a dict from features to counts\n",
    "          Usually, the count will just be 1.0 for\n",
    "          indicator functions.\n",
    "        \"\"\"\n",
    "        util.raiseNotDefined()\n",
    "\n",
    "class IdentityExtractor(FeatureExtractor):\n",
    "    def getFeatures(self, state, action):\n",
    "        feats = util.Counter()\n",
    "        feats[(state,action)] = 1.0\n",
    "        return feats\n",
    "\n",
    "class CoordinateExtractor(FeatureExtractor):\n",
    "    def getFeatures(self, state, action):\n",
    "        feats = util.Counter()\n",
    "        feats[state] = 1.0\n",
    "        feats['x=%d' % state[0]] = 1.0\n",
    "        feats['y=%d' % state[0]] = 1.0\n",
    "        feats['action=%s' % action] = 1.0\n",
    "        return feats\n",
    "\n",
    "def closestFood(pos, food, walls):\n",
    "    \"\"\"\n",
    "    closestFood -- this is similar to the function that we have\n",
    "    worked on in the search project; here its all in one place\n",
    "    \"\"\"\n",
    "    fringe = [(pos[0], pos[1], 0)]\n",
    "    expanded = set()\n",
    "    while fringe:\n",
    "        pos_x, pos_y, dist = fringe.pop(0)\n",
    "        if (pos_x, pos_y) in expanded:\n",
    "            continue\n",
    "        expanded.add((pos_x, pos_y))\n",
    "        # if we find a food at this location then exit\n",
    "        if food[pos_x][pos_y]:\n",
    "            return dist\n",
    "        # otherwise spread out from the location to its neighbours\n",
    "        nbrs = Actions.getLegalNeighbors((pos_x, pos_y), walls)\n",
    "        for nbr_x, nbr_y in nbrs:\n",
    "            fringe.append((nbr_x, nbr_y, dist+1))\n",
    "    # no food found\n",
    "    return None\n",
    "\n",
    "class SimpleExtractor(FeatureExtractor):\n",
    "    \"\"\"\n",
    "    Returns simple features for a basic reflex Pacman:\n",
    "    - whether food will be eaten\n",
    "    - how far away the next food is\n",
    "    - whether a ghost collision is imminent\n",
    "    - whether a ghost is one step away\n",
    "    \"\"\"\n",
    "\n",
    "    def getFeatures(self, state, action):\n",
    "        # extract the grid of food and wall locations and get the ghost locations\n",
    "        food = state.getFood()\n",
    "        walls = state.getWalls()\n",
    "        ghosts = state.getGhostPositions()\n",
    "\n",
    "        features = util.Counter()\n",
    "\n",
    "        features[\"bias\"] = 1.0\n",
    "\n",
    "        # compute the location of pacman after he takes the action\n",
    "        x, y = state.getPacmanPosition()\n",
    "        dx, dy = Actions.directionToVector(action)\n",
    "        next_x, next_y = int(x + dx), int(y + dy)\n",
    "\n",
    "        # count the number of ghosts 1-step away\n",
    "        features[\"#-of-ghosts-1-step-away\"] = sum((next_x, next_y) in Actions.getLegalNeighbors(g, walls) for g in ghosts)\n",
    "\n",
    "        # if there is no danger of ghosts then add the food feature\n",
    "        if not features[\"#-of-ghosts-1-step-away\"] and food[next_x][next_y]:\n",
    "            features[\"eats-food\"] = 1.0\n",
    "\n",
    "        dist = closestFood((next_x, next_y), food, walls)\n",
    "        if dist is not None:\n",
    "            # make the distance a number less than one otherwise the update\n",
    "            # will diverge wildly\n",
    "            features[\"closest-food\"] = float(dist) / (walls.width * walls.height)\n",
    "        features.divideAll(10.0)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qlearning_update.py\n",
    "def qlearning_update(self, state, action, nextState, reward):\n",
    "    difference = reward + self.discount * self.getValue(nextState) - self.getQValue(state, action)\n",
    "    features = self.featExtractor.getFeatures(state, action)\n",
    "    for key in features.keys():\n",
    "        self.weights[key] = self.weights[key] + self.alpha * difference * features[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p ApproximateQAgent -a extractor=SimpleExtractor -x 50 -n 60 -l mediumGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pacman.py -p ApproximateQAgent -a extractor=SimpleExtractor -x 50 -n 60 -l mediumClassic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "ایده‌هایی متفاوت‌تر!\n",
    "</font>\n",
    "همانطور که احتمالا می‌دانید، آنچه در این یادداشت باهم دیگر خواندیم و دیدیم، در واقع قطره‌ای از دریا بوده است.\n",
    "برای یاد گرفتن استراتژی بهینه می‌توان الگوریتم‌های بسیار زیادی را پیشنهاد کرد.\n",
    "Q-learning\n",
    "، که در واقع روشی بسیار مستقیم است تنها یکی از آن‌هاست.\n",
    "از طرف دیگر، روش‌های بسیار زیادی را می‌توان برای استخراج ویژگی‌ها از یک بازی استفاده کرد.\n",
    "    همانطور که در آخر این جزوه دیدیم، به صورت دستی و طبق شهود انسان‌ها ویژگی‌هایی را از بازی پک من استخراج کردیم که به نظر خودمان استراتژی‌های مفیدی بودند.\n",
    "    اما این چیزی نیست که خیلی جالب باشد زیرا در نهایت متکی به هوش انسانی شدیم.\n",
    "    برای رفع همین مشکل، همانطور که قبلا هم اشاره کردیم، می‌توان از روش‌های مختلفی مثل شبکه‌های عصبی بهره جست تا برای ما نمایش‌های مفیدی از وضعیت بازی به صورت خودکار استخراج کنند. \n",
    "    حتی در مورد جزییات می‌توان بسیار فکر کرد و خواند و جست و جو کرد.\n",
    "    برای مثال همانطور که در همین یادداشت مشاهده کردید مقادیر آلفا و گاما تاثیرات بسیاری بر بهینگی الگوریتم خواهند داشت.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br/>\n",
    "<div id=\"sec_refs\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<font color=#FF7500 size=6>\n",
    "منابع\n",
    "        </font>\n",
    "\t\t<hr>       \n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"http://ai.berkeley.edu\">AI-Berkeley</a>                                     \n",
    "            </li>\n",
    "        </ul>\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<br />\n",
    "<div id=\"packman\" style=\"direction:rtl;line-height:300%;\">\n",
    "\t\t<p></p>\n",
    "\t\t<hr>\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<font color=#FF7500 size=6>\n",
    "سخن پایانی\n",
    "</font>\n",
    "از صبر و حوصله‌ی شما بسیار سپاس‌گزاریم! امیدوارم تمام مشکلات این محتواها‌ را بر ما ببخشید. امیدواریم این محتوا، در سال‌های بعدی چالش هوش مصنوعی هر سال بالغ‌تر از سال قبل شوند و مطمئن هستیم که کمک شما در این راستا می‌تواند مفیدتر از هر چیز دیگری باشد. ما به هیچ‌وجه اصراری نداریم که این محتواها کامل، صحیح یا بدون غلط هستند و برای همین پذیرای هرگونه نظر شما هستیم! خواهشا ما را در این راه تنها نگذارید!\n",
    "    <br>\n",
    "    راه ارتباطی: <a href=\"mailto:aichallenge@sharif.edu\">aichallenge@sharif.edu</a>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
